<h1 align="center"> Desafio Ra√≠zen </h1>

## Reposit√≥rio destinado a constru√ß√£o do exerc√≠cio proposto, como forma de avalia√ß√£o da empresa Ra√≠zen

<h4 align="center"> 
	üöß  Ra√≠zen Project üöÄ Conclu√≠do com pontos de melhorias..  üöß
</h4>

## √çndice 

* [T√≠tulo](#t√≠tulo)
* [√çndice](#√≠ndice)
* [Descri√ß√£o do Projeto](#descri√ß√£o-do-projeto)
* [Tecnologias utilizadas](#tecnologias-utilizadas)
* [Resolu√ß√£o](#resolu√ß√£o)
* [Pontos de melhorias](#pontos-de-melhorias)
* [Execu√ß√£o do projeto](#execu√ß√£o-do-projeto)
* [Conclus√£o](#conclus√£o)

## Descri√ß√£o do Projeto

O projeto tem por objetivo, a resolu√ß√£o de um atividade proposta pela empresa Ra√≠zen, que consiste no desenvolvimento de uma pipeline ETL para extrair caches de piv√¥ de relat√≥rios consolidados, disponibilizados pela ag√™ncia reguladora de petr√≥leo/combust√≠veis do governo brasileiro, ANP(Ag√™ncia Nacional de Petr√≥leo, G√°s Natural e Biocombust√≠veis).
O arquivo em quest√£o possuem algumas pivot tables, onde √© necess√°ria a extra√ß√£o e manipula√ß√£o de duas dessas tabelas:
- Vendas de combust√≠veis derivados de petr√≥leo por UF e produto;
- Vendas de diesel por UF e tipo;

Os dados devem ser armazenados nos formatos a seguir

| Column       | Type        |
| ------------ | ----------- |
| `year_month` | `date`      |
| `uf`         | `string`    |
| `product`    | `string`    |
| `unit`       | `string`    |
| `volume`     | `double`    |
| `created_at` | `timestamp` |

Posteriormente definindo um schema de particionamento ou indexa√ß√£o.

Obs: (Parte da descri√ß√£o retirada do pr√≥prio arquivo da atividade).

## Tecnologias utilizadas 
:hammer:

- `Github`: Ferramenta utilizada para versionamento do projeto e reposit√≥rio; 
- `Python`: Linguagem para manipula√ß√£o dos arquivos e dados necess√°rios;
- `Docker`: Respons√°vel por criar a imagem necess√°ria para processamento da aplica√ß√£o;
- `Docker-compose`: Realiza a orquestra√ß√£o dos cont√™iners;
- `Airflow`: Organiza o fluxo de trabalho;

## Resolu√ß√£o

O primeiro desafio encontrado, foi como realizar a extra√ß√£o dos dados da "raw_data"(base de dados brutos), visto que n√£o era poss√≠vel em seu formato encontrado. Atrav√©s de algumas pesquisas, pode-se descobrir que fazendo a convers√£o da tabela do formato "xls" para "xlsx" poderia-se enfim extrair as tabelas necess√°rias para posterior manipula√ß√£o dos arquivos, sendo assim criado dois arquivos distintos xlsx, "oil_deritavite.xlsx" e "diesel.xlsx".
Fez-se ent√£o a leitura desses arquivos, adicionando-os em dataframe's para que fosse poss√≠vel fazer as manipula√ß√µes dos dados necess√°rias(ressaltando que o processo foi realizado em duas etapas, primeiro a leitura e processamento de um arquivo e posteriormente do outro). Ap√≥s realizar-se as tratativas necess√°rias e os dados estarem nos formatos solicitados, verificou-se a consist√™ncia desses dados em rela√ß√£o a "raw_data", concluindo que o processo foi realizado de forma correta, ent√£o optou-se por gravar esses dados em formato de parquet, sendo particionado por "product" e "year_month", concluido dessa forma a primeira parte do desenvolvimento.

O segundo passo no desenvolvimento do projeto, foi em qual plataforma fazer o gereciamento do fluxo de trabalho e como isso seria realizado. Levantados esses pontos, optou-se pela utiliza√ß√£o da ferramenta "Airflow", sendo utilizada atrav√©s do docker-compose pela praticidade que isso traria ao desenvolvimento da solu√ß√£o, assim como tamb√©m para execu√ß√£o. A plataforma do airflow foi iniciada atrav√©s de uma imagem criada dentro de um arquivo .yaml do docker-compose, sendo tamb√©m criado um Dockerfile para instala√ß√£o das configura√ß√µes necess√°rias para execu√ß√£o da aplica√ß√£o conteinerizada. 
No ambiente de desenvolvimento, criou-se as DAG's para orquestra√ß√£o do fluxo de trabalho, sendo estas vistas no ambiente do airflow atrav√©s do localhost da porta 8080.
Com isso conclui-se a etapa de controle do fluxo de trabalho atrav√©s de um ambiente conteinerizado.

## Pontos de melhorias

Existem alguns pontos que poderiam vir a ser melhorados no desenvolvimento do projeto, dentre eles podemos citar uma melhor performance no fluxo de trabalho, principalmete na etapa de extra√ß√£o dos dados das duas tabelas solicitadas, utilizou-se de "ferramentas" n√£o t√£o perform√°ticas no processo, sendo poss√≠vel uma melhor pesquisa por outras formas de se processar essa tarefa.
Outro ponto que vale ressaltar, seria aloca√ß√£o desses dados processados em um banco de dados para consultas, sendo poss√≠vel tamb√©m um backup para evitar o reprocesso, como em um ambiente produtivo √© default a replica√ß√£o por tr√™s(3) no HDFS.

## Execu√ß√£o do projeto 
üìÅ 

√â poss√≠vel realizar a execu√ß√£o do projeto de duas formas distintas, podendo elas serem realizadas no ambiente local ou em um ambiente conteinerizado.
- Para execu√ß√£o local, √© necess√°rio clonar o reposit√≥rio atrav√©s do comando:

	- git clone https://github.com/netomadazio/desafio_raizen.git

	Posteriormente entrar no diret√≥rio "local_excution" e executar o arquivo "ETL_Raizen_local.py":
	- cd local_excution
	- python ETL_Raizen_local.py

	Essa execu√ß√£o ir√° realizar todo a extra√ß√£o, manipula√ß√£o e carregamento dos parquet's das tabelas solicitadas localmente.

- Para execu√ß√£o no ambiente conteinerizado deve-se clonar o reposit√≥rio:

	- git clone https://github.com/netomadazio/desafio_raizen.git

	Subir o conteiner do airflow contendo as depend√™ncias do projeto: 
	- docker-compose up airflow-init
	- docker-compose up
	
	Acessar a porta no 8080 atrav√©s do navegador web:
	- localhost:8080
	
	Ent√£o executar o fluxo de trabalho.

## Conclus√£o

Atrav√©s do trabalho proposto pode-se desenvolver uma pipeline ETL, utilizando-se de algumas ferramentas dispon√≠veis no mercado, ficando como observa√ß√£o a possibilidade de realizar melhorias nas etapas executadas, construindo um melhor desenvolvimento, execu√ß√£o e entrega.
Agrade√ßo desde j√° pela oportunidade e sigo a disposi√ß√£o para quaisquer questionamentos.
Muito obrigado, Ra√≠zen.

Atenciosamente,

### Autor

Irineu Madazio Neto
Engenheiro de Controle e Automa√ß√£o 
apaixonado pela √°rea de Engenharia de Dados.

[![Linkedin Badge](https://img.shields.io/badge/-Irineu-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/irineu-madazio-neto/)](https://www.linkedin.com/in/irineu-madazio-neto/) 




	



