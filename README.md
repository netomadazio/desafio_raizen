<h1 align="center"> Desafio Ra√≠zen </h1>

## Reposit√≥rio destinado a constru√ß√£o do exerc√≠cio proposto, como forma de avalia√ß√£o da empresa Ra√≠zen

<h4 align="center"> 
	üöß  Ra√≠zen Project üöÄ Conclu√≠do com pontos de melhorias..  üöß
</h4>

## √çndice 

* [T√≠tulo](#T√≠tulo)
* [√çndice](#√≠ndice)
* [Descri√ß√£o do Projeto](#descri√ß√£o-do-projeto)
* [Tecnologias utilizadas](#tecnologias-utilizadas)
* [Resolu√ß√£o](#resolu√ß√£o)
* [Pontos de melhorias](#Pontos-de-melhorias)
* [Execu√ß√£o](#Execu√ß√£o)
* [Conclus√£o](#conclus√£o)

## Descri√ß√£o do Projeto

O projeto tem por objetivo, a resolu√ß√£o de um atividade proposta pela empresa Ra√≠zen, que consiste no desenvolvimento de uma pipeline ETL para extrair caches de piv√¥ de relat√≥rios consolidados, disponibilizados pela ag√™ncia reguladora de petr√≥leo/combust√≠veis do governo brasileiro, ANP(Ag√™ncia Nacional de Petr√≥leo, G√°s Natural e Biocombust√≠veis).
O arquivo em quest√£o possuem algumas pivot tables, onde √© necess√°ria a extra√ß√£o e manipula√ß√£o de duas dessas tabelas:
- Vendas de combust√≠veis derivados de petr√≥leo por UF e produto;
- Vendas de diesel por UF e tipo;

Os dados devem ser armazenados nos formatos a seguir

| Column       | Type        |
| ------------ | ----------- |
| `year_month` | `date`      |
| `uf`         | `string`    |
| `product`    | `string`    |
| `unit`       | `string`    |
| `volume`     | `double`    |
| `created_at` | `timestamp` |

Posteriormente definindo um schema de particionamento ou indexa√ß√£o.

Obs: (Parte da descri√ß√£o retirada do pr√≥prio arquivo da atividade).

## :hammer: Tecnologias utilizadas

- `Github`: Ferramenta utilizada para versionamento do projeto e reposit√≥rio; 
- `Python`: Linguagem para manipula√ß√£o dos arquivos e dados necess√°rios;
- `Docker`: Respons√°vel por criar a imagem necess√°ria para processamento da aplica√ß√£o;
- `Docker-compose`: Realiza a orquestra√ß√£o dos cont√™iners;
- `Airflow`: Organiza o fluxo de trabalho;

## Resolu√ß√£o

O primeiro desafio encontrado, foi como realizar a extra√ß√£o dos dados da "raw_data"(nossa base de dados brutos), visto que n√£o era poss√≠vel em seu formato encontrado. Atrav√©s de algumas pesquisas pode-se descobrir que fazendo a convers√£o da tabela do formato "xls" para "xlsx" poder√≠amos enfim extrair as tabelas necess√°rias para posterior manipula√ß√£o dos arquivos, sendo criado dois arquivos distintos xlsx, "oil_deritavite.xlsx" e "diesel.xlsx".
Fez-se ent√£o a leitura desses dois arquivos, adicionando-os em dataframe's para que fosse poss√≠vel fazer as manipula√ß√µes dos dados necess√°rias(ressaltando que o processo foi realizado em duas etapas, primeiro a leitura e processamento de um arquivo e posteriormente do outro). Ap√≥s realizar-se as tratativas necess√°rias e os dados estarem no formato solicitado, verificou-se a consist√™ncia desses dados em rela√ß√£o a "raw_data", concluindo que o processo foi realizado de forma correta, ent√£o optou-se por gravar esses dados em formato de parquet, sendo particionado por "product" e "year_month", concluido dessa forma a primeira parte do desenvolvimento.

O segundo passo no desenvolvimento do projeto, foi em qual plataforma fazer o gereciamento do fluxo de trabalho e como isso seria realizado. Levantados esses pontos, optou-se pela utiliza√ß√£o da ferramenta "Airflow", sendo utilizada atrav√©s do docker-compose pela praticidade que isso traria ao desenvolvimento da solu√ß√£o, assim como tamb√©m para execu√ß√£o. A plataforma do airflow foi iniciada atrav√©s de uma imagem criada dentro de um arquivo .yaml do docker-compose, sendo tamb√©m criado um Dockerfile para instala√ß√£o das configura√ß√µes necess√°rias para execu√ß√£o da aplica√ß√£o conteinerizada. 
No ambiente de desenvolvimento, criou-se as DAG's para orquestra√ß√£o do fluxo de trabalho, sendo estas vistas no ambiente do airflow atrav√©s do localhost da parta 8080.

## üìÅ Execu√ß√£o do projeto 
